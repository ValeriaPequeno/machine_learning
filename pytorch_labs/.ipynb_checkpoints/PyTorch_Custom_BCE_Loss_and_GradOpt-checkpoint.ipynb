{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fontanads/machine_learning/blob/master/pytorch_labs/PyTorch_Custom_BCE_Loss_and_GradOpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yQxMu0HJD5V6"
   },
   "source": [
    "# PyTorch Exercise: writing a custom loss function (BCE) and custom optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3tHY9qXEVtb"
   },
   "source": [
    "In this notebook, I create an artificial dataset aiming for binary classification of the datapoints.\n",
    "\n",
    "I implement Binary-Cross Entropy as a **custom loss function**. The custom loss function *inherits the backward-propagation* method from the main pytorch module.\n",
    "\n",
    "I also write a very simple **custom optimizer** to update the weights of the network, just to illustrate where to start from when you need to customize your update-rule equations.\n",
    "\n",
    "The dataset is in 2D for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOiyrHwQLXDi"
   },
   "source": [
    "**REMARK**: preferably, run the code with CUDA (GPU processing support for pytorch). <br>\n",
    "It can be done in Google Colab by setting the Runtime type to \"GPU\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pm5qPotooZc3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuD18kXMogx3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A61aBZP4ADOW"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JphBD5gsyFLO"
   },
   "outputs": [],
   "source": [
    "SEED = 42                             # random gen seed for numpy and torch\n",
    "use_custom_loss_and_opt = True        # set True if you want to use my custom functions or False for torch's loss and optimizer\n",
    "learning_rate = 1e-1                  # learning rate for parameter updates\n",
    "hidden_layer_sizes = [64, 32, 16]    # this is a list of the number of units in each Fully-Connected hidden layer, e.g. [16, 4, 2]\n",
    "max_epochs = 300                      # maximum number of iterations running over the training set\n",
    "train_size = 0.9                      # should be in the interval (0,1), it's the relative size of the training set\n",
    "M = 50000                             # this is the total number of data-points in the dataset\n",
    "print_status_iters = 10               # print loss val. every 'print_status_iters' iterations during training\n",
    "xlim, ylim = (10,10)                  # grid limits -xlim to xlim in the x-axis and -ylim to ylim in the y-axis\n",
    "normalize_X = True                    # a flag to normalize ANN's input data if set True\n",
    "CUDA_flag = torch.cuda.is_available() # a flat to check if CUDA is available for GPU use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_BLyvcixHTSP"
   },
   "source": [
    "Next cell is a function to implement data points in the 2D plane.\n",
    "\n",
    "Data-points $(x,y)$ satisfying the inequality\n",
    "\n",
    "$r^2 \\leq \\left[ \\dfrac{(x-x_0+n_x)\\cos\\theta+(y-y_0+n_y)\\sin\\theta}{a^2}\\right]^2 +\\left[ \\dfrac{(x-x_0+n_x)\\sin\\theta-(y-y_0+n_y)\\cos\\theta}{b^2}\\right]^2 \\leq R^2$ \n",
    "\n",
    "belong to the inside of the ellipse and will be labeled $1$. If the data point does not satisfy the inequality, it gets label $0$. \n",
    "\n",
    "Note that the values $n_x$ and $n_y$ are noise following Gaussian distribution $\\cal{N}(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0pIOojMot25"
   },
   "outputs": [],
   "source": [
    "def elliptical_dataset_gen_func(tuples_xy):\n",
    "    R = max(xlim,ylim)/2     # larger radius if circle\n",
    "    r = max(0,min(2.5,R-.1))   # smaller radius if circle\n",
    "    a,b = (1.2,.8)             # scaling factors of x and y, respectively  (increasing it causes x and y to shrink)\n",
    "    x0, y0= (-1, 1)            # ellipse center \n",
    "    sigma2 = 5e-2              # perturbation variance ('strength') that may cause a data-point to deviate from its intended (x,y) location \n",
    "    theta = 45*(np.pi/180)     # ellipse rotation angle with respect to the horizontal axis\n",
    "  \n",
    "    def noise_sample():\n",
    "        return np.sqrt(sigma2)*np.random.randn(1)\n",
    "\n",
    "    def offset(u,u0):\n",
    "        return (u -u0 + noise_sample())\n",
    "  \n",
    "    def factor(w,u,theta,s):\n",
    "        return (w*np.cos(theta) + u*np.sin(theta))**2/(s**2)\n",
    "  \n",
    "    def elipse(x,y):\n",
    "        w = offset(x,x0)\n",
    "        u = offset(y,y0)\n",
    "        return factor(w,u,theta,a) + factor(u,-w,theta,b)\n",
    "\n",
    "    dataset = [(x,y,1) if  r**2 <= elipse(x,y) <= R**2 else (x,y,0) for x,y in tuples_xy ]\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6H21wC3XKTnn"
   },
   "source": [
    "Next, we fill the 2D grid with the cartesian product of linearly spaced x and y vectors. \n",
    "\n",
    "The $(x,y)$ tuples are input to the datset generation, that outputs a list of tuples $(x,y,\\text{label})$.\n",
    "\n",
    "The figure shows the data of class $0$ (outside the ellipse) in blue while points of class $1$ (inside the ellipse) are in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "id": "F6fjuj9MA4bY",
    "outputId": "2f614897-b642-4268-a367-15a0fc3701ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c9f4729390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M_int = int(np.sqrt(M))\n",
    "np.random.seed(SEED)\n",
    "x = np.linspace(-xlim,xlim,M_int)\n",
    "y = np.linspace(-ylim,ylim,M_int)\n",
    "cartesian_xy = product(x,y)\n",
    "dataset = elliptical_dataset_gen_func(cartesian_xy)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(dataset[dataset[:,2]==0,0],dataset[dataset[:,2]==0,1],color='b')\n",
    "plt.scatter(dataset[dataset[:,2]==1,0],dataset[dataset[:,2]==1,1],color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZTWHYpGzAaEF",
    "outputId": "bf6ebea6-3b5f-4b3e-ef6e-fcbb81fdcf1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49729, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = torch.FloatTensor(dataset)\n",
    "if CUDA_flag:\n",
    "    dataset = dataset.cuda() # convert dataset to FloatTensor matrix (an then for CUDA)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vi_8_qphMKP7"
   },
   "source": [
    "Next is the design of the custom Artificial Neural Network (ANN) Model.\n",
    "\n",
    "It is created using a class, so that it inherits properties of torch's \"`nn.Module`\" class.\n",
    "\n",
    "In the initialization, the inputs are used to build the standard hidden layers. In this example, I fixed each hidden layer  to be\n",
    "\n",
    ">  $\\mathbf{h}=\\mathbf{W}^T\\mathbf{x}$ (*FC layer*) $\\to$ $\\mathbf{a}^\\prime =$ReLU$\\left(\\mathbf{h}\\right)$ $\\to$ $\\mathbf{a}$=BatchNorm$(\\mathbf{a})$\n",
    "\n",
    "and the final layer is just a wrap-up \n",
    "> ${h}=\\mathbf{w}^T\\mathbf{x}$ (*FC layer*) $\\to$ $\\hat{y}=\\sigma({h})$ (*Sigmoid activation*)\n",
    "\n",
    "so that the output is a scalar between $0$ and $1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_VaKn79pbwQ"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, layer_sizes):\n",
    "        super().__init__()\n",
    "        layers_list = []\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes:\n",
    "            layers_list.append(nn.Linear(n_in, n_out)) # FC layer\n",
    "            layers_list.append(nn.ReLU(inplace=True))  # ReLU activation function\n",
    "            layers_list.append(nn.BatchNorm1d(n_out))  # Batch Normalization\n",
    "            n_in = n_out\n",
    "        # layers_list.append(nn.Linear(layer_sizes[-1], n_classes)) # final layer for CrossEntropy\n",
    "        layers_list.append(nn.Linear(layer_sizes[-1], 1))           # final layer for BCE\n",
    "        layers_list.append(nn.Sigmoid())          # final activation function\n",
    "        self.layers = nn.Sequential(*layers_list) # network object  \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HUL2sWkbO4P7"
   },
   "source": [
    "Next, the loss function is defined.\n",
    "\n",
    "It is important to observe that:\n",
    "- the function must be writen using torch's Tensor operations and related functions\n",
    "- the output must be a scalar (loss)\n",
    "\n",
    "In this example, I chose to write the simple Binary Cross-Entropy for the evaluation of a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-iGo9z7qfz4"
   },
   "outputs": [],
   "source": [
    "class MyBynaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,output,target):\n",
    "        vec_ce = -target*torch.log(output + 1e-10) - (1-target)*torch.log(1-output + 1e-10) \n",
    "        return torch.mean( vec_ce )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5Mew2eqPaB4"
   },
   "source": [
    "Finally, the custom optimizer is described below.\n",
    "\n",
    "It inherits from torch's `'optim.Optimizer'` class.\n",
    "\n",
    "Initialization requires a dictionary with fixed hyperparameters. In this example, I only use the learning rate. For other optimizer, other hyperparameters such as `'betas'`, `'eps'`,  `'weight_decay'`, `'amsgrad'` may also be initialized.\n",
    "\n",
    "The **`step()`** method is where the network's parameter-update equations will be customized. The dictionary **`group`** holds the key \"params\" for the network adjustable parameters (e.g., the hidden-layer weights). \n",
    "\n",
    "Each parameter in `group['params']` holds its own gradient computed after backpropagation. This gradient is used to update its values, using the learning rate and other hyperparametrs initialized in the defaults dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p82c-JAiqjXp"
   },
   "outputs": [],
   "source": [
    "class MyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, model_parameters, lr=1e-1):\n",
    "        self.lr = lr \n",
    "        self.params = model_parameters\n",
    "        defaults = {'lr':lr} # dictionary following structure group_name:param_values\n",
    "        super().__init__(model_parameters,defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p=p.grad\n",
    "                p.data.add_(-group['lr']*d_p) \n",
    "                # remark: I had trouble using the in-place operation in the leaf-variable p, \n",
    "                # so I modified to update the data of the tensor directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vfy1heqMTaqv"
   },
   "source": [
    "From here on, standard torch training and test procedures take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juvgQi8XThBs"
   },
   "source": [
    "In the next cell, I use torch's `random_split` function to get generators for the train and test set.\n",
    "\n",
    "Since the dataset is not so large, I alredy extracted each subset into `(X_train, y_train)` and `(X_test, y_test)` variables.\n",
    "\n",
    "An optional normalization is also used for the input variables. But note that it depends only on the `train_set` to avoid *data leakage*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0V_4IyLIArbt"
   },
   "outputs": [],
   "source": [
    "train_len = int(train_size*dataset.shape[0])\n",
    "test_len = dataset.shape[0]-train_len\n",
    "train_set, test_set = random_split(dataset, [train_len,test_len])\n",
    "\n",
    "X_train = train_set[:][:,:-1]\n",
    "\n",
    "norm_factor = torch.max(X_train.std(dim=0)) if normalize_X else 1 \n",
    "\n",
    "X_train = X_train/norm_factor\n",
    "y_train = train_set[:][:,-1]\n",
    "X_test  = test_set[:][:,:-1]/norm_factor\n",
    "y_test  = test_set[:][:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yu_C_wmnVE-Q"
   },
   "source": [
    "Next, the custom Model is instantiated.\n",
    "\n",
    "Depending on what setting you have chosen, custom loss function and custom optimizer will be used. \n",
    "\n",
    "Otherwise, torch's `BCELoss()` and `SGD()` optimizer will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Wlu1CGm_XA9"
   },
   "outputs": [],
   "source": [
    "Model = MyModel(X_train.shape[1], 2, hidden_layer_sizes)\n",
    "if CUDA_flag:\n",
    "    Model.cuda()\n",
    "\n",
    "if not use_custom_loss_and_opt:\n",
    "    criterion = nn.BCELoss(   ) \n",
    "    optimizer = torch.optim.SGD(Model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    criterion = MyBynaryCrossEntropyLoss( )\n",
    "    optimizer = MyOptimizer(Model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "9JwWIY6g5k9B",
    "outputId": "22f3d5c4-2315-4d2e-9b68-7aedeb37343b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MyModel(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "Criterion: MyBynaryCrossEntropyLoss()\n",
      "\n",
      "Optimizer: MyOptimizer (\n",
      "Parameter Group 0\n",
      "    lr: 0.1\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f'Model {Model}',f'Criterion: {criterion}',f'Optimizer: {optimizer}',sep=2*'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIxcVCrTVgL_"
   },
   "source": [
    "Here is the standard torch training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "TMLOUbD-BfRp",
    "outputId": "c8a19eaf-801c-44b6-e374-bf1f7803d68a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 and loss is 0.725211 (runtime:     0.5176 sec)\n",
      "epoch 11 and loss is 0.487241 (runtime:     3.5728 sec)\n",
      "epoch 21 and loss is 0.358021 (runtime:     7.6528 sec)\n",
      "epoch 31 and loss is 0.293639 (runtime:    11.6773 sec)\n",
      "epoch 41 and loss is 0.222525 (runtime:    15.8251 sec)\n",
      "epoch 51 and loss is 0.247636 (runtime:    20.0554 sec)\n",
      "epoch 61 and loss is 0.144879 (runtime:    24.5027 sec)\n",
      "epoch 71 and loss is 0.163707 (runtime:    28.7834 sec)\n",
      "epoch 81 and loss is 0.124294 (runtime:    32.6931 sec)\n",
      "epoch 91 and loss is 0.129747 (runtime:    36.6642 sec)\n",
      "epoch 101 and loss is 0.105070 (runtime:    40.4851 sec)\n",
      "epoch 111 and loss is 0.102465 (runtime:    44.3991 sec)\n",
      "epoch 121 and loss is 0.104466 (runtime:    48.2593 sec)\n",
      "epoch 131 and loss is 0.100329 (runtime:    51.9813 sec)\n",
      "epoch 141 and loss is 0.086920 (runtime:    55.8575 sec)\n",
      "epoch 151 and loss is 0.088689 (runtime:    59.8847 sec)\n",
      "epoch 161 and loss is 0.084527 (runtime:    63.5995 sec)\n",
      "epoch 171 and loss is 0.076399 (runtime:    67.3275 sec)\n",
      "epoch 181 and loss is 0.072274 (runtime:    71.4625 sec)\n",
      "epoch 191 and loss is 0.069064 (runtime:    75.7816 sec)\n",
      "epoch 201 and loss is 0.088851 (runtime:    80.0578 sec)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "losses=[]\n",
    "start_time = time.time()\n",
    "for i in range(max_epochs):\n",
    "    y_pred = Model(X_train).reshape(train_len) # forward-prop in network\n",
    "    loss = criterion(y_pred,y_train )          # loss calculation\n",
    "    losses.append(loss)                        # save loss\n",
    "    optimizer.zero_grad()                      # reset gradients\n",
    "    loss.backward()                            # backprop\n",
    "    optimizer.step()                           # params update\n",
    "    \n",
    "    if i%print_status_iters==1: # print status\n",
    "        duration = time.time() - start_time \n",
    "        print(f'epoch {i} and loss is {loss:5.6f} (runtime: {duration:10.4f} sec)')\n",
    "        \n",
    "duration = time.time() - start_time \n",
    "print(f'train took {duration/60} min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rZtGnSmHTxmL",
    "outputId": "bfa58246-9ad1-4d1a-99e3-8508ff289a2a"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DtExkyanVqsU"
   },
   "source": [
    "Plot of the loss function during training (y-axis in `log` scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "colab_type": "code",
    "id": "iZz53A_EC-En",
    "outputId": "5955af32-ddfa-4556-9f41-ac138be1fb43"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(max_epochs), losses)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('LOSS')\n",
    "plt.yscale('log')\n",
    "plt.ylim([1e-3,1e0])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6oAz7qZVzqn"
   },
   "source": [
    "Here, predictions are made with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "L3hW2aWLHuqd",
    "outputId": "843048e7-de45-47a7-be84-13ef549fbb81"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_val = Model(X_test).reshape(test_len)\n",
    "    loss  = criterion(y_val, y_test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxBk_7XPV6ja"
   },
   "source": [
    "Then we plot the true values vs. the predictions. \n",
    "\n",
    "Note that the Sigmoid's output is a soft-value between $0$ and $1$.\n",
    "\n",
    "We can make a hard decision by rouding then or we can plot the confidence using a color gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jo_7w_w54cdm"
   },
   "outputs": [],
   "source": [
    "#y_pred = torch.argmax(y_val, axis=1).cpu().numpy() # cross-entropy output needs 2 classes\n",
    "y_pred_hard = y_val.round().cpu().numpy()\n",
    "y_pred_soft = y_val.cpu().numpy().round(decimals=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "a6NlEpNma_nO",
    "outputId": "2f403e39-7f6e-4f55-dfe0-666e687fc397"
   },
   "outputs": [],
   "source": [
    "plt.hist(y_pred_soft) # just a histogram to quick view the distribution of soft prediction-values\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('sigmoid output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IHUGMfXOKimW",
    "outputId": "8eb929e3-7275-44ea-9941-d73910521292"
   },
   "outputs": [],
   "source": [
    "gs = gridspec.GridSpec(2, 2) # Create 2x2 sub plots\n",
    "\n",
    "plt.figure(figsize=(12, 9), dpi=100, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.subplot(gs[0, 0])\n",
    "plt.scatter(X_test.cpu()[y_test==0,0],X_test.cpu()[y_test==0,1],color='b')\n",
    "plt.scatter(X_test.cpu()[y_test==1,0],X_test.cpu()[y_test==1,1],color='r')\n",
    "plt.xlabel('x'), plt.ylabel('y')\n",
    "plt.title('True Classes')\n",
    "\n",
    "plt.subplot(gs[0, 1])\n",
    "plt.scatter(X_test.cpu()[y_pred_hard==0,0],X_test.cpu()[y_pred_hard==0,1],color='b')\n",
    "plt.scatter(X_test.cpu()[y_pred_hard==1,0],X_test.cpu()[y_pred_hard==1,1],color='r')\n",
    "plt.xlabel('x'), plt.ylabel('y')\n",
    "plt.title('Classification Model Hard-Predictions')\n",
    "\n",
    "plt.subplot(gs[1, 1])\n",
    "plt.scatter(X_test.cpu()[y_pred_hard==0,0],X_test.cpu()[y_pred_hard==0,1],color='b')\n",
    "for i in range(len(y_pred_soft)):\n",
    "    plt.scatter(X_test.cpu()[i,0],X_test.cpu()[i,1],color=(y_pred_soft[i],0,1-y_pred_soft[i]))\n",
    "plt.xlabel('x'), plt.ylabel('y')\n",
    "plt.title('Classification Model Soft-Predictions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9mo0hrsXBaf"
   },
   "source": [
    "Here we evaluate the classification metric.\n",
    "\n",
    "First, a confusion matrix is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4GCbdZ_LO_l"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBrxfz05PiCW"
   },
   "outputs": [],
   "source": [
    "y_true = y_test.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGkq0dQLXLTf"
   },
   "source": [
    "In each row of the confusion matrix, we see absolute number of hits and errors.\n",
    "\n",
    "Note that in row $0$, the first item is the number of hits, \n",
    "\n",
    "while in row $1$, the second item is the number of hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UXrF44RrOleX",
    "outputId": "7ebcadfc-3d24-4c72-b7f8-fad286b86225"
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zlSY-78MXjPd"
   },
   "source": [
    "Instead of absolute number of erros, we can view the confusion matrix with the percentage of hits and errors for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "UhWvaASsNZT4",
    "outputId": "485a3a40-aef0-41fc-ec8b-49aa1bd66815"
   },
   "outputs": [],
   "source": [
    "np.around(100*confusion_matrix(y_true, y_pred_hard, normalize='true'),decimals=2) # row (label) normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_sjD941Xrwq"
   },
   "source": [
    "Other classification metrics are reported with the following `sklearn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "cQyySboMPes2",
    "outputId": "bcfc1715-5d81-4928-d744-a3b84e60aa07"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(y_true, y_pred_hard, target_names=['0','1'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PydPM0eDX0_B"
   },
   "source": [
    "At last, some examples of predicted  vs. True classification values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "id": "pjn8GOrIIL9-",
    "outputId": "94ff4bfe-633c-4d72-bee0-25c499fa2d5c"
   },
   "outputs": [],
   "source": [
    "for i in np.random.randint(low=0, high=test_len, size=10):\n",
    "    pred_val  = y_pred_hard[i] # torch.argmax(y_val[i])\n",
    "    soft_val  = y_pred_soft[i]\n",
    "    true_val  = y_test[i].long()\n",
    "    print(f'PREDICTED (HARD/SOFT): {int(pred_val)}/{soft_val:.2f} TRUE: {true_val}  | Input.: {i}) \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOLjIvqZCENDL6h9fnJakG1",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PyTorch_Custom_BCE_Loss_and_GradOpt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
