{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating NLP Word Embeddings with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/50747947/embedding-in-pytorch\n",
    "# https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wikipedia\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "#!pip install --user \"git+https://github.com/javadba/mpld3@display_fix\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraper collects corpus from Wikipedia in real time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Wikipedia_Crawler` class uses the `wikipedia` module to load a _corpus_ in real time.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_labs.my_nlp_classes import Wikipedia_Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to set the desired language of the searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang('pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will search specific wikis from the input query and downloads its content.\n",
    "- `query_list` is the list of query text to look for in Wikipedia\n",
    "- `max_results` is the maximum number of pages to load per query expression in the `query_list`\n",
    "- `max_v` will be used later to limit the vocabulary size of our custom dictionary\n",
    "- `skip_top` is the number of most frequent words (descending order) to be discard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_list  = ['Rainha','Homem','História','Europa','Monarquia'] #  \n",
    "max_results = 1\n",
    "max_V       = 2000\n",
    "skip_top    = 0\n",
    "max_len_sentence = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell, we instantiate the crawler with its basic query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler = Wikipedia_Crawler(query_list=query_list,max_results_per_query=max_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the desired wiki pages, we call for `query_wiki`. <br>\n",
    "The methods `get_wiki_sentences` downloads and tokenizes the sentences of the pages. <br> \n",
    "Then, `get_all_tokens` lists all unique tokens (words) in the data. Tokenization is performed in the process. <br>\n",
    "Finally, `count_vocabulary` counts the absolute frequency of each token. Tokenization is performed in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler.query_wiki()\n",
    "wiki_crawler.get_wiki_sentences()\n",
    "wiki_crawler.get_all_tokens()\n",
    "wiki_crawler.count_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all the tokens may be prohibitive. Thus, it is necessary to limit the vocabulary. <br>\n",
    "- `generate_vocabulary` will create `word2idx` and `idx2word` dictionaries \n",
    "- - dictionaries will be limited with `max_V` words ($+ 3$ tags)\n",
    "- - note that `skip_top` most frequent words will be discarded\n",
    "- `encode_all_sentence` will encode sentences of words into sequences (lists) of corresponding indexes\n",
    "- - encoded sentences will have additional `\"<START>\"` and `\"<END>\"` tokens\n",
    "- - words out of vocabulary will be coded with index of `\"<OOV>\"` tag\n",
    "- - encoded senteces will be truncated with `max_len_sentence` actual words ($+ 2$ start/end tags)\n",
    "- - encoded senteces padded by default, unless `pad_sentences` is set to `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_crawler.generate_vocabulary(max_len_vocabulary=max_V, skip_top_words=skip_top)\n",
    "wiki_crawler.encode_all_sentence(max_len_sentence=max_len_sentence, pad_sentences=True) # pad_sentences is True by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check where did the corpus came from and other results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Corpus derived from the following wiki pages: \n",
      " ---- ['Rainha', 'Homem', 'História']\n",
      " Number of sentences: ------ 131\n",
      " Max. words in sentence: --- 70\n",
      " Vocabulary Size: ---------- 1003\n"
     ]
    }
   ],
   "source": [
    "S = len(wiki_crawler.sentences)           \n",
    "T = len(wiki_crawler.coded_sentences[0]) # max_len_sentence + 2\n",
    "V = len(wiki_crawler.word2idx)           # max_V + 2\n",
    "\n",
    "print(f' Corpus derived from the following wiki pages: \\n ---- {wiki_crawler.titles_list}')\n",
    "print(f' Number of sentences: ------ {S}')\n",
    "print(f' Max. words in sentence: --- {T-2}')\n",
    "print(f' Vocabulary Size: ---------- {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can check the generated sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Real Text sentence: \n",
      "    entre os reis davi de judá e israel, não é mencionado uma única rainha reinante; apesar de atália, embora a bíblia se refira negativamente como uma usurpadora.\n",
      "\n",
      " Encoded Sentence: \n",
      "    [1, 107, 9, 252, 458, 3, 459, 5, 460, 33, 13, 461, 16, 152, 26, 37, 462, 3, 463, 108, 4, 253, 14, 464, 465, 20, 16, 466, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " Decoded Sentence: \n",
      "    <START> entre os reis davi de judá e israel não é mencionado uma única rainha reinante apesar de atália embora a bíblia se refira negativamente como uma usurpadora <END>\n"
     ]
    }
   ],
   "source": [
    "num_sentence = 5\n",
    "\n",
    "real_sentence    = wiki_crawler.sentences[num_sentence]\n",
    "encoded_sentece  = wiki_crawler.coded_sentences[num_sentence]\n",
    "decoded_sentence = wiki_crawler.decode_one_sentence(encoded_sentece)\n",
    "\n",
    "print(f' Real Text sentence: \\n    {real_sentence}', end=2*'\\n')\n",
    "print(f' Encoded Sentence: \\n    {encoded_sentece}', end=2*'\\n')\n",
    "print(f' Decoded Sentence: \\n    '+' '.join(decoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can see how to use the dictionary directly to convert `wor2idx` or `idx2word`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word \"mulher\" corresponds to index 85\n",
      " Index 9 corresponds to word \"os\"\n"
     ]
    }
   ],
   "source": [
    "word = 'mulher'\n",
    "idx = 9\n",
    "print(f' Word \"{word}\" corresponds to index {wiki_crawler.word2idx[word]}')\n",
    "print(f' Index {idx} corresponds to word \"{wiki_crawler.idx2word[idx]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll see how to use the ``Embedding`` layer of Torch. <br>\n",
    "It converts categorical data with $V$ classes to dense vectors with $N_d$ dimensions. <br>\n",
    "Suppose $c\\in{F_2^V}$ is a one-hot encoded vector. <br>\n",
    "An embedding is a mapping $e:F_2^V\\to R^{N_d}$ (sparse vector, one-hot encoded, to dense real vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd = 2                  # Number of Dimensions of the Dense embedding\n",
    "e = nn.Embedding(V,Nd)  # (vocab_size, num_of_dimensions_of_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first convert the coded sentences to Torch Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " V x S : torch.Size([72, 131]) (vocab. size x num. of sentences)\n",
      " Tensors of type torch.int64\n",
      " All encoded sentences as tensors (each column is a sentence):\n",
      "tensor([[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "        [26, 26, 10,  ..., 61, 61, 61],\n",
      "        [78, 37, 15,  ..., 62, 62, 62],\n",
      "        ...,\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "coded_sentences = torch.LongTensor(wiki_crawler.coded_sentences).reshape(-1,T).T\n",
    "print(f' V x S : {coded_sentences.shape} (vocab. size x num. of sentences)')\n",
    "print(f' Tensors of type {coded_sentences.dtype}')\n",
    "print(' All encoded sentences as tensors (each column is a sentence):')\n",
    "print(coded_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select, for example, the first sentence, as the varible $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72])\n",
      "tensor([  1,  26,  78, 417,   4, 418,  13,   6, 237,   3,  38, 419, 420, 421,\n",
      "         36,  64,  20,   4,  26, 238,  39, 422, 239,   8, 240, 423,  21, 424,\n",
      "         36, 241,   8,  38, 104, 242,  79,   3,  26,  65,  39,  26, 425,   7,\n",
      "        143,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n"
     ]
    }
   ],
   "source": [
    "c = coded_sentences[:,0]\n",
    "print(c.shape,c,sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert the sentence from sequences of one-hot-encoded words to its embedding vector.<br>\n",
    "When printing the corresponding sequence, we see that:\n",
    "- each index (one-hot-encoded word) is converted into a real row-vector of $N_d$ dimensions\n",
    "- the initialized embedding vectors are just random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72, 2])\n",
      "tensor([[-0.6759, -0.2747],\n",
      "        [ 0.7723,  0.4207],\n",
      "        [ 1.3175, -0.0512],\n",
      "        [ 0.0698,  1.6624],\n",
      "        [ 1.1524,  0.1323],\n",
      "        [ 0.2990, -0.3231],\n",
      "        [ 1.4584,  2.1189],\n",
      "        [-0.5233, -0.5855],\n",
      "        [-0.1794, -0.3164],\n",
      "        [-0.6383, -0.2990]], grad_fn=<SliceBackward>)\n",
      "... and more words (truncated in the 10-th word).\n"
     ]
    }
   ],
   "source": [
    "e_seq = e(c)\n",
    "print(e_seq.shape,e_seq[:10,:],'... and more words (truncated in the 10-th word).',sep='\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a word to check its embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Word \"rainha\" corresponds to index 26\n",
      " Index 26 maps to embedding vector \"[0.772270143032074, 0.4206901490688324]\"\n"
     ]
    }
   ],
   "source": [
    "word    = 'rainha'\n",
    "\n",
    "idx     = wiki_crawler.word2idx[word] # uses dictionary to map word to index\n",
    "emb_vec = e(torch.LongTensor([idx]))  # uses torch embedding to map index to dense Nd-vector\n",
    "\n",
    "print(f' Word \"{word}\" corresponds to index {idx}')\n",
    "print(f' Index {idx} maps to embedding vector \"{emb_vec.detach().numpy().reshape(-1).tolist()}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003, 2]),\n",
       " torch.float32,\n",
       " tensor([[-0.7519, -0.3386],\n",
       "         [-0.6759, -0.2747],\n",
       "         [-1.4828, -1.5623],\n",
       "         ...,\n",
       "         [ 0.4125,  1.7732],\n",
       "         [-0.4292,  0.5969],\n",
       "         [ 0.0863, -2.6722]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = e.weight.data\n",
    "W1.shape, W1.dtype, W1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Embeddings with Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag-of-Words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context vs target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  1,  26,  78, 417,   4, 418,  13,   6, 237,   3,  38, 419, 420, 421,\n",
      "         36,  64,  20,   4,  26, 238,  39, 422, 239,   8, 240, 423,  21, 424,\n",
      "         36, 241,   8,  38, 104, 242,  79,   3,  26,  65,  39,  26, 425,   7,\n",
      "        143,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0])\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Left context: [], Middle word: 26, right context [78, 417, 4, 418]\n",
      " Full context: [78, 417, 4, 418]\n",
      " Words in context of ['rainha']: ['antigo', 'ray', 'a', 'reynna']\n",
      "\n",
      " Left context: [26], Middle word: 78, right context [417, 4, 418]\n",
      " Full context: [26, 417, 4, 418]\n",
      " Words in context of ['antigo']: ['rainha', 'ray', 'a', 'reynna']\n",
      "\n",
      " Left context: [26, 78], Middle word: 417, right context [4, 418]\n",
      " Full context: [26, 78, 4, 418]\n",
      " Words in context of ['ray']: ['rainha', 'antigo', 'a', 'reynna']\n",
      "\n",
      " Left context: [78, 417], Middle word: 4, right context [418, 13]\n",
      " Full context: [78, 417, 418, 13]\n",
      " Words in context of ['a']: ['antigo', 'ray', 'reynna', 'é']\n",
      "\n",
      " Left context: [26, 65], Middle word: 39, right context [26, 425]\n",
      " Full context: [26, 65, 26, 425]\n",
      " Words in context of ['exemplo']: ['rainha', 'consorte', 'rainha', 'silvia']\n",
      "\n",
      " Left context: [65, 39], Middle word: 26, right context [425, 7]\n",
      " Full context: [65, 39, 425, 7]\n",
      " Words in context of ['rainha']: ['consorte', 'exemplo', 'silvia', 'da']\n",
      "\n",
      " Left context: [39, 26], Middle word: 425, right context [7, 143]\n",
      " Full context: [39, 26, 7, 143]\n",
      " Words in context of ['silvia']: ['exemplo', 'rainha', 'da', 'suécia']\n",
      "\n",
      " Left context: [39, 26, 425], Middle word: 7, right context [143]\n",
      " Full context: [39, 26, 425, 143]\n",
      " Words in context of ['da']: ['exemplo', 'rainha', 'silvia', 'suécia']\n",
      "\n",
      " Left context: [39, 26, 425, 7], Middle word: 143, right context []\n",
      " Full context: [39, 26, 425, 7]\n",
      " Words in context of ['suécia']: ['exemplo', 'rainha', 'silvia', 'da']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_size = 2\n",
    "end_position = (c==2).nonzero().item()\n",
    "\n",
    "samples_to_print = np.concatenate((np.arange(1,5),np.arange(end_position-5,end_position)),axis=0)\n",
    "for pos in samples_to_print: # range(1,end_position):\n",
    "    \n",
    "    middle_word   = c[pos]\n",
    "    \n",
    "    '''if middle_word==wiki_crawler.word2idx['<END>']:\n",
    "        break'''\n",
    "    \n",
    "    left_start = max(pos-context_size,1)\n",
    "    right_end  = min(pos+1+context_size,end_position)\n",
    "    \n",
    "    \n",
    "    if pos - left_start < context_size:                # first words position\n",
    "        right_end += context_size-(pos-left_start)     # --> increse right_context size\n",
    "    elif right_end-(pos+1) < context_size:             # last words position\n",
    "        left_start -= context_size-(right_end-(pos+1)) # --> increase left_context size\n",
    "            \n",
    "    left_context  = c[left_start:pos]\n",
    "    right_context = c[pos+1:right_end]\n",
    "    \n",
    "    context       = torch.cat((left_context,right_context),dim=0)\n",
    "    \n",
    "    print(f' Left context: {left_context.tolist()}, Middle word: {middle_word.tolist()}, right context {right_context.tolist()}')\n",
    "    print(f' Full context: {context.tolist()}')\n",
    "    \n",
    "    context_decoded     = wiki_crawler.decode_one_sentence(context.tolist())\n",
    "    middle_word_decoded = wiki_crawler.decode_one_sentence([middle_word.tolist()])\n",
    "    print(f' Words in context of {middle_word_decoded}: {context_decoded}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(coded_sentence, context_size):\n",
    "    \n",
    "    end_position = (c==2).nonzero().item()\n",
    "    target_context = []\n",
    "    for pos in range(1,end_position):\n",
    "        middle_word   = coded_sentence[pos]\n",
    "\n",
    "        left_start = max(pos-context_size,1)\n",
    "        right_end  = min(pos+1+context_size,end_position)\n",
    "\n",
    "        if pos - left_start < context_size:                # first words position\n",
    "            right_end += context_size-(pos-left_start)     # --> increse right_context size\n",
    "        elif right_end-(pos+1) < context_size:             # last words position\n",
    "            left_start -= context_size-(right_end-(pos+1)) # --> increase left_context size\n",
    "\n",
    "        left_context  = c[left_start:pos]\n",
    "        right_context = c[pos+1:right_end]\n",
    "        \n",
    "        context       = torch.cat((left_context,right_context),dim=0)\n",
    "        \n",
    "        target_context.append((middle_word.view((-1)),context))\n",
    "    return target_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_Model_w2v(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, num_of_dimensions_of_embedding, context_size):\n",
    "        super(Language_Model_w2v, self).__init__()\n",
    "        self.V  = vocab_size\n",
    "        self.N  = num_of_dimensions_of_embedding \n",
    "        self.cs = context_size\n",
    "        \n",
    "        self.e  = nn.Embedding(self.V, self.N)\n",
    "        '''\n",
    "        self.W1 = nn.Linear(2*self.cs*self.N,128)\n",
    "        self.W2 = nn.Linear(128, self.V)\n",
    "        '''\n",
    "        # self.layers = nn.Sequential(*[self.e, self.W1, self.W2,self.LogSoftMax])\n",
    "        \n",
    "        self.Winv = nn.Linear(self.N, self.V)\n",
    "        self.LogSoftMax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self,x):               # x is 2*cs x 1\n",
    "        x = self.e(x)\n",
    "        x = x.sum(dim=0).view(1,-1)\n",
    "        x = self.Winv(x)\n",
    "        x = self.LogSoftMax(x)\n",
    "        \n",
    "        return x\n",
    "        ''' \n",
    "        x = self.e(x).view((1,-1))     # e(x) 2*cs x N\n",
    "        x = self.W1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.W2(x)\n",
    "        log_probs = self.LogSoftMax(x)\n",
    "        return log_probs\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "print_iters = 50\n",
    "cs = 3\n",
    "Nd = 10\n",
    "losses    = []\n",
    "criterion = nn.NLLLoss()\n",
    "model     = Language_Model_w2v(V, num_of_dimensions_of_embedding=Nd, context_size=cs)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3) # optim.SGD(model.parameters(),lr=1e-3) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Language_Model_w2v(\n",
       "  (e): Embedding(1003, 10)\n",
       "  (Winv): Linear(in_features=10, out_features=1003, bias=True)\n",
       "  (LogSoftMax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    cnt = 0\n",
    "    total_loss = 0\n",
    "    \n",
    "    coded_sentences = coded_sentences[:,np.random.permutation(S)]\n",
    "    \n",
    "    for n in range(S):\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words into integer indices and wrap them in tensors)\n",
    "        c = coded_sentences[:,n]\n",
    "        \n",
    "        pos_cnt = 0\n",
    "        for target, context in get_context(c, cs):\n",
    "            pos_cnt+=1\n",
    "            if len(context)<2*cs:\n",
    "                continue\n",
    "            \n",
    "            # Step 2. Recall that torch *accumulates* gradients. \n",
    "            # Before passing in a new instance, you need to zero out the gradients from the oldinstance\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Step 3. Run the forward pass, getting log probabilities over next words\n",
    "            log_probs = model(context)  \n",
    "            \n",
    "            # Step 4. Compute your loss function. \n",
    "            # (Again, Torch wants the target word wrapped in a tensor)\n",
    "            loss = criterion(log_probs, target)\n",
    "\n",
    "            # Step 5. Do the backward pass and update the gradient\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "            total_loss += loss.item()/T/S\n",
    "            \n",
    "        cnt+=1\n",
    "        if n%print_iters==1:\n",
    "            print(f'epoch {epoch} sentence {n}/{S} loss: {total_loss*(T/pos_cnt):.5f}')\n",
    "    print(f'loss: {total_loss:.6f}')\n",
    "    losses.append(total_loss)\n",
    "    \n",
    "    time.sleep(1.5)\n",
    "    clear_output()\n",
    "#print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "log_probs.data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for group in model.parameters():\n",
    "    print(group.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a770f74f98>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/html": [
       "\n",
       "\n",
       "<style>\n",
       "\n",
       "</style>\n",
       "\n",
       "<div id=\"fig_el304818186660496647109318266\"></div>\n",
       "<script>\n",
       "function mpld3_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(mpld3) !== \"undefined\" && mpld3._mpld3IsLoaded){\n",
       "   // already loaded: just create the figure\n",
       "   !function(mpld3){\n",
       "       \n",
       "       mpld3.draw_figure(\"fig_el304818186660496647109318266\", {\"width\": 792.0, \"height\": 288.0, \"axes\": [{\"bbox\": [0.125, 0.125, 0.775, 0.755], \"xlim\": [-0.2, 4.2], \"ylim\": [1.731208838082442, 2.401069270828425], \"xdomain\": [-0.2, 4.2], \"ydomain\": [1.731208838082442, 2.401069270828425], \"xscale\": \"linear\", \"yscale\": \"linear\", \"axes\": [{\"position\": \"bottom\", \"nticks\": 11, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}, {\"position\": \"left\", \"nticks\": 9, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"zoomable\": true, \"id\": \"el30481818666053584\", \"lines\": [{\"data\": \"data01\", \"xindex\": 0, \"yindex\": 1, \"coordinates\": \"data\", \"id\": \"el30481818666422168\", \"color\": \"#1F77B4\", \"linewidth\": 1.5, \"dasharray\": \"none\", \"alpha\": 1, \"zorder\": 2, \"drawstyle\": \"default\"}], \"paths\": [], \"markers\": [], \"texts\": [], \"collections\": [], \"images\": [], \"sharex\": [], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 2.3706210693399714], [1.0, 2.020967586240887], [2.0, 1.898959894038176], [3.0, 1.8205130399252978], [4.0, 1.7616570395708957]]}, \"id\": \"el30481818666049664\", \"plugins\": [{\"type\": \"reset\"}, {\"type\": \"zoom\", \"button\": true, \"enabled\": false}, {\"type\": \"boxzoom\", \"button\": true, \"enabled\": false}]});\n",
       "   }(mpld3);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/mpld3\n",
       "   require.config({paths: {d3: \"https://mpld3.github.io/js/d3.v3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.3.1.dev1.js\", function(){\n",
       "         \n",
       "         mpld3.draw_figure(\"fig_el304818186660496647109318266\", {\"width\": 792.0, \"height\": 288.0, \"axes\": [{\"bbox\": [0.125, 0.125, 0.775, 0.755], \"xlim\": [-0.2, 4.2], \"ylim\": [1.731208838082442, 2.401069270828425], \"xdomain\": [-0.2, 4.2], \"ydomain\": [1.731208838082442, 2.401069270828425], \"xscale\": \"linear\", \"yscale\": \"linear\", \"axes\": [{\"position\": \"bottom\", \"nticks\": 11, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}, {\"position\": \"left\", \"nticks\": 9, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"zoomable\": true, \"id\": \"el30481818666053584\", \"lines\": [{\"data\": \"data01\", \"xindex\": 0, \"yindex\": 1, \"coordinates\": \"data\", \"id\": \"el30481818666422168\", \"color\": \"#1F77B4\", \"linewidth\": 1.5, \"dasharray\": \"none\", \"alpha\": 1, \"zorder\": 2, \"drawstyle\": \"default\"}], \"paths\": [], \"markers\": [], \"texts\": [], \"collections\": [], \"images\": [], \"sharex\": [], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 2.3706210693399714], [1.0, 2.020967586240887], [2.0, 1.898959894038176], [3.0, 1.8205130399252978], [4.0, 1.7616570395708957]]}, \"id\": \"el30481818666049664\", \"plugins\": [{\"type\": \"reset\"}, {\"type\": \"zoom\", \"button\": true, \"enabled\": false}, {\"type\": \"boxzoom\", \"button\": true, \"enabled\": false}]});\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & mpld3\n",
       "    mpld3_load_lib(\"https://mpld3.github.io/js/d3.v3.min.js\", function(){\n",
       "         mpld3_load_lib(\"https://mpld3.github.io/js/mpld3.v0.3.1.dev1.js\", function(){\n",
       "                 \n",
       "                 mpld3.draw_figure(\"fig_el304818186660496647109318266\", {\"width\": 792.0, \"height\": 288.0, \"axes\": [{\"bbox\": [0.125, 0.125, 0.775, 0.755], \"xlim\": [-0.2, 4.2], \"ylim\": [1.731208838082442, 2.401069270828425], \"xdomain\": [-0.2, 4.2], \"ydomain\": [1.731208838082442, 2.401069270828425], \"xscale\": \"linear\", \"yscale\": \"linear\", \"axes\": [{\"position\": \"bottom\", \"nticks\": 11, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}, {\"position\": \"left\", \"nticks\": 9, \"tickvalues\": null, \"tickformat\": null, \"scale\": \"linear\", \"fontsize\": 10.0, \"grid\": {\"gridOn\": false}, \"visible\": true}], \"axesbg\": \"#FFFFFF\", \"axesbgalpha\": null, \"zoomable\": true, \"id\": \"el30481818666053584\", \"lines\": [{\"data\": \"data01\", \"xindex\": 0, \"yindex\": 1, \"coordinates\": \"data\", \"id\": \"el30481818666422168\", \"color\": \"#1F77B4\", \"linewidth\": 1.5, \"dasharray\": \"none\", \"alpha\": 1, \"zorder\": 2, \"drawstyle\": \"default\"}], \"paths\": [], \"markers\": [], \"texts\": [], \"collections\": [], \"images\": [], \"sharex\": [], \"sharey\": []}], \"data\": {\"data01\": [[0.0, 2.3706210693399714], [1.0, 2.020967586240887], [2.0, 1.898959894038176], [3.0, 1.8205130399252978], [4.0, 1.7616570395708957]]}, \"id\": \"el30481818666049664\", \"plugins\": [{\"type\": \"reset\"}, {\"type\": \"zoom\", \"button\": true, \"enabled\": false}, {\"type\": \"boxzoom\", \"button\": true, \"enabled\": false}]});\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<Figure size 792x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e.weight torch.Size([1003, 10])\n",
      "Winv.weight torch.Size([1003, 10])\n",
      "Winv.bias torch.Size([1003])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape) # param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003, 10]),\n",
       " tensor([[-0.3198, -0.2899, -2.2958,  ..., -0.3903, -0.4486, -1.3068],\n",
       "         [ 2.9330,  0.0376,  0.0094,  ...,  0.2946, -0.1227,  0.3848],\n",
       "         [-0.4135,  1.1940, -0.5218,  ...,  0.4835, -0.1628, -1.6356],\n",
       "         ...,\n",
       "         [-1.1270,  0.3880,  0.5045,  ..., -0.5396,  0.3963,  0.5688],\n",
       "         [-2.0240,  0.7019, -0.5027,  ...,  0.1783, -0.0449, -0.5614],\n",
       "         [-0.8073,  2.4165,  0.9969,  ..., -0.0555, -0.2019,  0.7703]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding = model.e.weight.data\n",
    "Embedding.shape, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(pos_neg_1=('king','man'),pos_neg_2=('queen','woman'), n_closest = 4):\n",
    "    king_w,    man_w = pos_neg_1\n",
    "    queen_w, woman_w = pos_neg_2\n",
    "    \n",
    "    for w in (king_w,man_w,woman_w):\n",
    "        if w not in wiki_crawler.word2idx:\n",
    "            raise Exception(f'sorry, word \"{w}\" not in dictionary.')\n",
    "    \n",
    "    print(f'Expected: {king_w} - {man_w} = {queen_w} - {woman_w}')\n",
    "    \n",
    "    king   = wiki_crawler.word2idx[king_w]\n",
    "    man    = wiki_crawler.word2idx[man_w]\n",
    "    woman  = wiki_crawler.word2idx[woman_w]\n",
    "    \n",
    "    vec = (Embedding[king] - Embedding[man] + Embedding[woman]).view(1,-1) # Embedding[queen]\n",
    "    distances = pairwise_distances(vec.reshape(1, -1), Embedding, metric='cosine').reshape(V)\n",
    "    \n",
    "    idx = distances.argsort()[:n_closest+3] \n",
    "    idx = [x for x in idx if x not in set([man,king,woman])]\n",
    "    queen_estimated = wiki_crawler.idx2word[idx[0]]\n",
    "    \n",
    "    print(f'Got:      {king_w} - {man_w} = {queen_estimated} - {woman_w}')\n",
    "    \n",
    "    print(f'Closest {len(idx)} words:')\n",
    "    for i in idx:\n",
    "        print(f'{wiki_crawler.idx2word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: rei - homem = rainha - mulher\n",
      "Got:      rei - homem = exibem - mulher\n",
      "Closest 7 words:\n",
      "exibem\n",
      "metodológico\n",
      "das\n",
      "graus\n",
      "entrevistas\n",
      "referir\n",
      "político\n"
     ]
    }
   ],
   "source": [
    "word_analogy(pos_neg_1=('rei','homem'),pos_neg_2=('rainha','mulher'), n_closest = 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_negative_sampling_distribution(coded_sentences, V):\n",
    "    # Pn(w) = prob of word occuring\n",
    "    # we would like to sample the negative samples such that \n",
    "    # words that occur more often should be sampled more often\n",
    "    \n",
    "    freq  = np.zeros(V)                 # vector with V positions\n",
    "    for c in coded_sentences:\n",
    "        for w in c:\n",
    "            freq[w] += 1          # it is assumed that each word is actually its OHE index\n",
    "\n",
    "    f_neg = freq**0.75     # smoothing\n",
    "    pmf_neg = f_neg / f_neg.sum() # normalized pmf\n",
    "    \n",
    "    assert(np.all(pmf_neg > 0)) # check that only positive probabilities were drawn\n",
    "    \n",
    "    return pmf_neg\n",
    "\n",
    "# distribution for drawing negative samples\n",
    "pmf_neg = get_negative_sampling_distribution(wiki_crawler.coded_sentences, V) \n",
    "\n",
    "# for subsampling each sentence\n",
    "threshold = 1e-5 # 1 em 100.000\n",
    "p_drop = 1 - np.sqrt(threshold / pmf_neg)  # prob. de ignorar uma palavra :: aumenta conforme a frequência dessa palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
